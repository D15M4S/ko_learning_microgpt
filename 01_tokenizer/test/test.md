# 01. Tokenizer - AI Engineer 면접 테스트

> 실제 AI Engineer 면접에서 나올 수 있는 질문들입니다.
> 답변을 작성한 후 `answer.md`와 비교하세요.

---

## 📝 테스트 방법
1. 각 질문을 읽고 **종이에** 또는 **마크다운 파일에** 답변 작성
2. 시간 제한: 각 섹션당 표시된 시간 내에 완료
3. 답변 완료 후 `answer.md` 확인
4. 틀린 부분은 `notes.md` 또는 `deepdive_BPE.md`에서 다시 복습

---

## Section 1: 기본 개념 (10분)

### Q1.1 토크나이저 정의
**질문:** 토크나이저(Tokenizer)란 무엇이며, 왜 필요한가요?

**답변:**
```
(여기에 답변 작성)
```

---

### Q1.2 토크나이저 분류
**질문:** 토크나이저를 3가지로 분류하고, 각각의 특징을 표로 정리하세요.

**답변:**
```
| 종류 | 어휘 크기 | 시퀀스 길이 | OOV 문제 | 예시 모델 |
|------|----------|------------|---------|----------|
|      |          |            |         |          |
|      |          |            |         |          |
|      |          |            |         |          |
```

---

### Q1.3 BOS 토큰
**질문:** BOS 토큰이란 무엇이며, 왜 필요한가요? 3가지 이유를 설명하세요.

**답변:**
```
1.
2.
3.
```

---

## Section 2: 코드 구현 (15분)

### Q2.1 어휘 생성 코드 분석
**질문:** 다음 코드의 각 줄이 무엇을 하는지 설명하세요.
```python
uchars = sorted(set(''.join(docs)))
BOS = len(uchars)
vocab_size = len(uchars) + 1
```

**답변:**
```
Line 1:

Line 2:

Line 3:
```

---

### Q2.2 인코딩 함수 구현
**질문:** 다음 요구사항을 만족하는 `encode` 함수를 작성하세요.
- 입력: 문자열 `text`, 어휘 `uchars`, BOS 토큰 ID `BOS`
- 출력: 토큰 ID 리스트 `[BOS, id1, id2, ..., BOS]`

**답변:**
```python
def encode(text, uchars, BOS):
    # 여기에 구현
    pass
```

---

### Q2.3 디코딩 함수 구현
**질문:** 다음 요구사항를 만족하는 `decode` 함수를 작성하세요.
- 입력: 토큰 ID 리스트 `tokens`, 어휘 `uchars`, BOS 토큰 ID `BOS`
- 출력: 원본 문자열 (BOS 토큰 제외)

**답변:**
```python
def decode(tokens, uchars, BOS):
    # 여기에 구현
    pass
```

---

## Section 3: 손으로 계산 (20분)

### Q3.1 어휘 생성
**질문:** 다음 데이터셋이 주어졌을 때, `uchars`, `BOS`, `vocab_size`를 계산하세요.
```python
docs = ["cat", "dog", "bat"]
```

**답변:**
```
uchars =
BOS =
vocab_size =
```

---

### Q3.2 인코딩 계산
**질문:** Q3.1의 어휘를 사용하여 "dog"를 인코딩하세요.

**답변:**
```
'd' → index ?
'o' → index ?
'g' → index ?

tokens = [?, ?, ?, ?, ?]
```

---

### Q3.3 디코딩 계산
**질문:** Q3.1의 어휘를 사용하여 `[6, 1, 2, 6]`을 디코딩하세요.

**답변:**
```
6 → ?
1 → ?
2 → ?
6 → ?

result = "?"
```

---

## Section 4: BPE 알고리즘 (25분)

### Q4.1 BPE 정의
**질문:** BPE (Byte Pair Encoding)란 무엇이며, Character-level 토크나이저와 어떻게 다른가요?

**답변:**
```
BPE란?


Character-level과의 차이:
1.
2.
3.
```

---

### Q4.2 BPE 학습 과정
**질문:** BPE 학습 알고리즘의 4단계를 설명하세요.

**답변:**
```
1. 초기화:

2. 쌍 빈도 계산:

3. 선택:

4. 병합 및 반복:
```

---

### Q4.3 BPE 손계산
**질문:** 다음 코퍼스로 BPE 학습을 3번 반복하세요.
```
"low" (빈도: 5)
"newest" (빈도: 6)
```

**답변:**
```
초기 상태:
l o w </w> (5)
n e w e s t </w> (6)

=== 병합 1 ===
쌍 빈도 계산:


선택한 쌍:

병합 후:


=== 병합 2 ===
쌍 빈도 계산:


선택한 쌍:

병합 후:


=== 병합 3 ===
쌍 빈도 계산:


선택한 쌍:

병합 후:

```

---

## Section 5: 실전 시나리오 (20분)

### Q5.1 토크나이저 선택
**질문:** 다음 시나리오에서 어떤 토크나이저를 선택할지, 그 이유를 설명하세요.

**시나리오 1:** 교육용 미니 GPT 구현 (200줄 이내 코드)
```
선택:
이유:
```

**시나리오 2:** 프로덕션 레벨 다국어 번역 모델
```
선택:
이유:
```

**시나리오 3:** 영어 단어 자동완성 시스템
```
선택:
이유:
```

---

### Q5.2 성능 비교
**질문:** "internationalization"이라는 단어를 처리할 때:
1. Character-level: 몇 개 토큰?
2. BPE (GPT-2 기준): 예상 토큰 수와 분할 예시
3. Word-level: 문제점은?

**답변:**
```
1. Character-level:


2. BPE:


3. Word-level:

```

---

### Q5.3 메모리 계산
**질문:** 다음 설정에서 토큰 임베딩 테이블의 메모리 사용량을 계산하세요.
```
vocab_size = 50,000
embedding_dim = 768
dtype = float32 (4 bytes)
```

**답변:**
```
계산:


결과:
```

---

## Section 6: 디버깅 & 최적화 (15분)

### Q6.1 버그 찾기
**질문:** 다음 코드의 버그를 찾고 수정하세요.
```python
def encode(text, uchars, BOS):
    tokens = [uchars.index(ch) for ch in text]
    return tokens
```

**답변:**
```
버그 1:

버그 2:

수정된 코드:

```

---

### Q6.2 성능 문제
**질문:** 다음 코드의 성능 문제를 지적하고 개선 방법을 제안하세요.
```python
# 1억 개 문서 처리
all_chars = []
for doc in docs:  # 1억 개
    for char in doc:
        if char not in all_chars:
            all_chars.append(char)
```

**답변:**
```
문제점:


개선 방법:


개선된 코드:

```

---

### Q6.3 엣지 케이스
**질문:** 다음 상황에서 토크나이저가 어떻게 동작해야 하는지 설명하세요.

**케이스 1:** 빈 문자열 ""
```
기대 동작:
```

**케이스 2:** 어휘에 없는 문자 (예: 이모지 😀)
```
기대 동작:
```

**케이스 3:** 매우 긴 단어 (1000자 이상)
```
기대 동작:
```

---

## Section 7: 심화 질문 (20분)

### Q7.1 GPT-2 vs GPT-3 토크나이저
**질문:** GPT-2와 GPT-3의 토크나이저 차이점을 설명하세요.

**답변:**
```
GPT-2:


GPT-3:


주요 차이점:

```

---

### Q7.2 Byte-level BPE
**질문:** Byte-level BPE가 Character-level BPE보다 나은 이유는 무엇인가요?

**답변:**
```


```

---

### Q7.3 토크나이저와 모델 성능
**질문:** 토크나이저의 어휘 크기가 모델 성능에 미치는 영향을 설명하세요. (어휘가 너무 작을 때, 너무 클 때)

**답변:**
```
어휘가 너무 작을 때 (예: 100):
장점:
단점:

어휘가 너무 클 때 (예: 1,000,000):
장점:
단점:

최적 크기:
```

---

## Section 8: Python 지식 (10분)

### Q8.1 `set()` vs `list`
**질문:** `set()`과 `list`의 차이점을 5가지 이상 나열하세요.

**답변:**
```
1.
2.
3.
4.
5.
```

---

### Q8.2 `''.join()` 성능
**질문:** 왜 `''.join(list)`이 `+` 연산자보다 빠른가요? 시간 복잡도로 설명하세요.

**답변:**
```
+ 연산자:


''.join():


이유:
```

---

### Q8.3 리스트 컴프리헨션
**질문:** 다음 리스트 컴프리헨션을 for 루프로 변환하세요.
```python
tokens = [uchars.index(ch) for ch in text if ch in uchars]
```

**답변:**
```python
# 여기에 for 루프 작성

```

---

## Section 9: 시스템 디자인 (15분)

### Q9.1 대규모 토크나이저 설계
**질문:** 1억 개의 문서를 토크나이징해야 합니다. 시스템을 어떻게 설계하시겠습니까?

**답변:**
```
1. 데이터 전처리:


2. 병렬 처리:


3. 메모리 관리:


4. 성능 최적화:

```

---

### Q9.2 다국어 지원
**질문:** 영어, 한글, 일본어, 중국어를 모두 지원하는 토크나이저를 설계하세요.

**답변:**
```
접근 방법:


기술 선택:


주의사항:

```

---

## 🎯 채점 기준

### 점수 계산
- Section 1-3: 각 문제 5점 × 총 9문제 = 45점
- Section 4: 각 문제 10점 × 3문제 = 30점
- Section 5-9: 각 문제 5점 × 15문제 = 75점
- **총점: 150점**

### 등급
- **140점 이상**: Senior level - 완벽한 이해
- **120-139점**: Mid level - 실무 가능
- **100-119점**: Junior level - 기본기 갖춤
- **80-99점**: Entry level - 추가 학습 필요
- **80점 미만**: 복습 필요

---

## 📚 학습 자료
- 기본 개념은 [notes.md](../notes.md) 재학습
- BPE 관련 질문은 [deepdive_BPE.md](../deepdive_BPE.md) 참고
- 손계산 연습은 notes.md의 예제로 반복

---

**정답 확인:** [answer.md](./answer.md)
